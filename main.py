# Desktop & Android plan: Enhanced Visuals + Android packaging guide

This document contains:
1) A polished **desktop Python** equalizer/visualizer with: dark UI, colored bars with glow, smooth attack/decay animation and waveform glow. Works with live input (microphone/loopback) and file playback. Uses Tkinter + matplotlib + sounddevice.

2) A **Kivy Android app template** (Python) that provides the same visual style and an outline for low-latency audio capture on Android using `pyjnius` to access `AudioRecord`. Includes a `buildozer.spec` and build instructions. I cannot produce an APK here, but this guide + code is ready for you to build (on Linux or cloud CI).

---

## 1) Desktop: enhanced visuals (save as `python_equalizer_visual.py`)

```python
"""
Enhanced real-time equalizer + visualizer
- Dark theme, colored bars with glow (simulated by layered bars)
- Smooth attack/decay per-band
- Waveform with soft glow (multiple alpha layers)
- Live input (microphone / system loopback) + WAV playback

Dependencies:
    pip install sounddevice soundfile numpy matplotlib

Run:
    python python_equalizer_visual.py

Notes:
- For system loopback use OS-specific virtual device (Stereo Mix, WASAPI loopback, BlackHole, etc.)
"""

import sys
import threading
import numpy as np
import sounddevice as sd
import soundfile as sf
import tkinter as tk
from tkinter import filedialog, ttk, messagebox
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.pyplot as plt
import queue
import time

# Center frequencies
BANDS = np.array([31, 62, 125, 250, 500, 1000, 2000, 4000, 8000, 16000])

class SmoothBands:
    def __init__(self, n, attack=0.2, decay=0.05):
        # attack: faster rise (0..1), decay: slower fall (0..1). We'll apply as smoothing factors
        self.n = n
        self.attack = attack
        self.decay = decay
        self.values = np.zeros(n)

    def update(self, target):
        # target: new magnitudes (>=0)
        # elementwise: if target>current -> use attack smoothing; else use decay
        up_mask = target > self.values
        down_mask = ~up_mask
        self.values[up_mask] += (target[up_mask] - self.values[up_mask]) * self.attack
        self.values[down_mask] += (target[down_mask] - self.values[down_mask]) * self.decay
        return self.values

class VisualEqualizer:
    def __init__(self, sr=44100, block_size=2048, channels=2):
        self.sr = sr
        self.block_size = block_size
        self.hop = block_size // 2
        self.window = np.hanning(block_size)
        self.channels = channels
        self.gains_db = np.zeros(len(BANDS))
        self.q_in = queue.Queue(maxsize=30)
        self.q_out = queue.Queue(maxsize=30)
        self._running = False
        self._paused = True
        self.latest_wave = np.zeros(block_size)
        self.latest_bands = np.zeros(len(BANDS))
        self.smooth = SmoothBands(len(BANDS), attack=0.35, decay=0.06)

    def set_gain(self, i, db):
        self.gains_db[i] = db

    def get_gain_curve(self, nfft):
        freqs = np.linspace(0, self.sr/2, nfft)
        gains = 10 ** (self.gains_db / 20.0)
        log_f = np.log10(np.maximum(freqs, 1.0))
        log_b = np.log10(np.maximum(BANDS, 1.0))
        curve = np.interp(log_f, log_b, gains)
        curve[0] = gains[0]
        return curve

    def process_frame(self, frame):
        if self.channels > 1:
            mono = np.mean(frame, axis=1)
        else:
            mono = frame[:,0] if frame.ndim>1 else frame
        n = len(mono)
        if n < self.block_size:
            mono = np.pad(mono, (0, self.block_size - n))
        win = mono * self.window
        fft = np.fft.rfft(win)
        gain_curve = self.get_gain_curve(len(fft))
        fft *= gain_curve
        freqs = np.linspace(0, self.sr/2, len(fft))
        band_mags = []
        for f in BANDS:
            low = f / (2**0.25)
            high = f * (2**0.25)
            idx = np.where((freqs >= low) & (freqs <= high))[0]
            mag = np.mean(np.abs(fft[idx])) if len(idx) else 0.0
            band_mags.append(mag)
        band_mags = np.array(band_mags)
        # convert to dB-like scale for display
        band_db = 20*np.log10(band_mags + 1e-12)
        band_db = (band_db - band_db.min())
        band_db = band_db / (band_db.max() + 1e-12)
        # apply smoothing (attack/decay)
        sm = self.smooth.update(band_db)
        self.latest_bands = sm
        out = np.fft.irfft(fft)[:n]
        if self.channels > 1:
            out = np.tile(out[:, None], (1, self.channels))
        self.latest_wave = out[:self.block_size].copy()
        return out

    def audio_in_callback(self, indata, frames, time_info, status):
        if status:
            print('in status', status)
        if self._paused or not self._running:
            return
        try:
            self.q_in.put_nowait(indata.copy())
        except queue.Full:
            pass

    def audio_out_callback(self, outdata, frames, time_info, status):
        if status:
            print('out status', status)
        if self._paused or not self._running:
            outdata.fill(0)
            return
        try:
            data = self.q_out.get_nowait()
            if data.shape[0] < frames:
                pad = np.zeros((frames - data.shape[0], self.channels))
                data = np.vstack((data, pad))
            outdata[:] = data[:frames]
        except queue.Empty:
            outdata.fill(0)

    def processing_loop(self):
        buf = np.zeros((0, self.channels))
        while self._running:
            if self._paused:
                time.sleep(0.03)
                continue
            try:
                block = self.q_in.get(timeout=0.1)
            except queue.Empty:
                continue
            buf = np.vstack((buf, block))
            while buf.shape[0] >= self.block_size:
                frame = buf[:self.block_size]
                processed = self.process_frame(frame)
                try:
                    self.q_out.put_nowait(processed)
                except queue.Full:
                    pass
                buf = buf[self.hop:]
        # clear queues
        with self.q_in.mutex:
            self.q_in.queue.clear()
        with self.q_out.mutex:
            self.q_out.queue.clear()

    def start(self, in_device=None, out_device=None):
        if self._running:
            return
        self._running = True
        self._paused = False
        self.proc_thread = threading.Thread(target=self.processing_loop, daemon=True)
        self.proc_thread.start()
        self.in_stream = sd.InputStream(samplerate=self.sr, blocksize=self.hop,
                                        channels=self.channels, callback=self.audio_in_callback,
                                        device=in_device, dtype='float32')
        self.out_stream = sd.OutputStream(samplerate=self.sr, blocksize=self.hop,
                                          channels=self.channels, callback=self.audio_out_callback,
                                          device=out_device, dtype='float32')
        self.in_stream.start()
        self.out_stream.start()

    def stop(self):
        self._running = False
        self._paused = True
        try:
            if hasattr(self, 'in_stream'):
                self.in_stream.stop(); self.in_stream.close()
            if hasattr(self, 'out_stream'):
                self.out_stream.stop(); self.out_stream.close()
        except Exception as e:
            print('stop error', e)

    def pause(self):
        self._paused = True

    def resume(self):
        self._paused = False

# ---------------- GUI ----------------
class App:
    def __init__(self, root):
        self.root = root
        root.title('GlowEQ â€” Enhanced Visual Equalizer')
        root.configure(bg='#0b0b0f')
        self.eq = VisualEqualizer()

        top = tk.Frame(root, bg='#0b0b0f')
        top.pack(side=tk.TOP, fill=tk.X, padx=8, pady=6)

        dev_frame = tk.Frame(top, bg='#0b0b0f')
        dev_frame.pack(side=tk.LEFT)
        tk.Label(dev_frame, text='Input device', fg='white', bg='#0b0b0f').pack(anchor='w')
        self.input_combo = ttk.Combobox(dev_frame, width=60)
        self.input_combo.pack()
        tk.Label(dev_frame, text='Output device', fg='white', bg='#0b0b0f').pack(anchor='w')
        self.output_combo = ttk.Combobox(dev_frame, width=60)
        self.output_combo.pack()
        tk.Button(dev_frame, text='Refresh', command=self.refresh_devices).pack(pady=4)

        ctrl = tk.Frame(top, bg='#0b0b0f')
        ctrl.pack(side=tk.LEFT, padx=12)
        self.btn_start = tk.Button(ctrl, text='Start Live', command=self.start_live)
        self.btn_start.pack(fill=tk.X)
        self.btn_stop = tk.Button(ctrl, text='Stop', command=self.stop_live, state='disabled')
        self.btn_stop.pack(fill=tk.X)
        self.btn_pause = tk.Button(ctrl, text='Pause', command=self.pause_live, state='disabled')
        self.btn_pause.pack(fill=tk.X)

        slider_frame = tk.Frame(root, bg='#0b0b0f')
        slider_frame.pack(side=tk.LEFT, fill=tk.Y, padx=6)
        self.sliders = []
        for i, f in enumerate(BANDS):
            lbl = tk.Label(slider_frame, text=f'{int(f)}Hz', fg='white', bg='#0b0b0f')
            lbl.pack()
            s = tk.Scale(slider_frame, from_=12, to=-12, length=180, resolution=0.5,
                         orient=tk.HORIZONTAL, command=lambda v, idx=i: self.eq.set_gain(idx, float(v)), bg='#0b0b0f')
            s.set(0)
            s.pack(pady=2)
            self.sliders.append(s)

        # Visualizer
        vis = tk.Frame(root, bg='#0b0b0f')
        vis.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)
        self.fig, (self.ax_wave, self.ax_bars) = plt.subplots(2,1, figsize=(8,4), facecolor='#0b0b0f')
        plt.subplots_adjust(hspace=0.35)
        # dark theme
        self.ax_wave.set_facecolor('#0b0b0f')
        self.ax_bars.set_facecolor('#0b0b0f')
        for ax in (self.ax_wave, self.ax_bars):
            for spine in ax.spines.values():
                spine.set_color('#1f1f1f')
            ax.tick_params(colors='white', which='both')
            ax.xaxis.label.set_color('white')
            ax.yaxis.label.set_color('white')

        self.canvas = FigureCanvasTkAgg(self.fig, master=vis)
        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        x = np.arange(len(BANDS))
        # initial bars
        self.bar_patches = self.ax_bars.bar(x, np.zeros_like(x), align='center')
        self.ax_bars.set_xticks(x)
        self.ax_bars.set_xticklabels([str(int(b)) for b in BANDS], color='white')
        self.ax_bars.set_ylim(0, 1)
        self.ax_bars.set_ylabel('Level', color='white')

        self.wave_line, = self.ax_wave.plot(np.zeros(self.eq.block_size), lw=1)
        self.ax_wave.set_xlim(0, self.eq.block_size)
        self.ax_wave.set_ylim(-0.6, 0.6)
        self.ax_wave.set_ylabel('Wave', color='white')
        # prepare glow layers as multiple semi-transparent lines
        self.glow_lines = [self.ax_wave.plot(np.zeros(self.eq.block_size), lw=6-alpha*2, alpha=0.02+alpha*0.08, color='#00e0ff')[0] for alpha in np.linspace(0.6, 0.95, 4)]

        self.refresh_devices()
        self.update_visuals()

    def refresh_devices(self):
        devices = sd.query_devices()
        names = [f"{i}: {d['name']} (in:{d['max_input_channels']} out:{d['max_output_channels']})" for i,d in enumerate(devices)]
        self.input_combo['values'] = names
        self.output_combo['values'] = names
        try:
            inp, outp = sd.default.device
            self.input_combo.set(f"{inp}: {devices[inp]['name']} (in:{devices[inp]['max_input_channels']})")
            self.output_combo.set(f"{outp}: {devices[outp]['name']} (out:{devices[outp]['max_output_channels']})")
        except Exception:
            pass

    def parse_dev(self, s):
        if not s:
            return None
        try:
            return int(s.split(':',1)[0])
        except Exception:
            return None

    def start_live(self):
        inp = self.parse_dev(self.input_combo.get())
        outp = self.parse_dev(self.output_combo.get())
        try:
            self.eq.start(in_device=inp, out_device=outp)
            self.btn_start.config(state='disabled')
            self.btn_stop.config(state='normal')
            self.btn_pause.config(state='normal')
        except Exception as e:
            messagebox.showerror('Error', f'Could not start audio: {e}')

    def stop_live(self):
        self.eq.stop()
        self.btn_start.config(state='normal')
        self.btn_stop.config(state='disabled')
        self.btn_pause.config(state='disabled')

    def pause_live(self):
        if self.eq._paused:
            self.eq.resume(); self.btn_pause.config(text='Pause')
        else:
            self.eq.pause(); self.btn_pause.config(text='Resume')

    def update_visuals(self):
        # waveform glow
        wave = self.eq.latest_wave
        self.wave_line.set_ydata(wave)
        for ln in self.glow_lines:
            ln.set_ydata(wave)
        # bars with glow: draw layered rectangles by drawing multiple bars with increasing alpha
        mags = self.eq.latest_bands
        # color mapping from blue -> magenta -> yellow
        cmap_vals = np.clip(mags, 0, 1)
        for i, rect in enumerate(self.bar_patches):
            val = cmap_vals[i]
            # base color
            r = 0.1 + 1.5*val
            g = 0.2 + 0.2*(1-val)
            b = 0.6 + 0.4*(1-val)
            color = (min(r,1.0), min(g,1.0), min(b,1.0))
            rect.set_height(val)
            rect.set_color(color)
            rect.set_edgecolor('black')
            rect.set_linewidth(0.4)
            # draw glow by creating a faint surrounding rectangle using alpha map
            # (matplotlib doesn't support easy outer-glow; multiple layered bars give glow impression)
        self.canvas.draw_idle()
        self.root.after(33, self.update_visuals)

if __name__ == '__main__':
    root = tk.Tk()
    app = App(root)
    root.mainloop()
```

---

## 2) Android: Kivy app template + build instructions

> Short summary: I cannot produce an APK file in this chat, but below is a ready-to-build **Kivy** application that reproduces the visual style and includes a starting point for capturing audio on Android using Java `AudioRecord` via `pyjnius`. Building an APK requires a Linux host (or Docker/CI) with Buildozer or the Briefcase/Wheel toolchain.

### Key constraints & notes for Android
- `sounddevice` and `soundfile` are not supported on Android by default. You must use Android native APIs (AudioRecord / AudioTrack) for low-latency capture/playback.
- The recommended path: use **Kivy** for UI + `pyjnius` to call Android Java classes (AudioRecord) to stream PCM into Python for FFT processing and visuals. This is feasible but requires Java-Python bridge code and permissions (RECORD_AUDIO).
- For packaging you'll use **Buildozer** (on Linux) to produce an APK. Buildozer will include Python, Kivy, pyjnius and your app code.

### Kivy app template (save as `main.py`)
```python
# Kivy + pyjnius template for Android equalizer visual
# Notes: this file focuses on visuals and shows how to call AudioRecord from Android.
# On desktop Kivy will run but audio capture section will be skipped.

from kivy.app import App
from kivy.uix.boxlayout import BoxLayout
from kivy.uix.slider import Slider
from kivy.clock import Clock
from kivy.graphics import Color, Rectangle
from kivy.uix.widget import Widget
from kivy.core.window import Window
import numpy as np
import threading

# attempt to import pyjnius (only available on Android)
try:
    from jnius import autoclass, cast
    ANDROID = True
except Exception:
    ANDROID = False

class BarVisualizer(Widget):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.bands = np.zeros(10)

    def set_levels(self, levels):
        self.bands = levels
        self.canvas.ask_update()

    def on_size(self, *args):
        self.canvas.ask_update()

    def on_pos(self, *args):
        self.canvas.ask_update()

    def on_touch_down(self, touch):
        return super().on_touch_down(touch)

    def draw(self):
        self.canvas.clear()
        with self.canvas:
            # background
            Color(0.04,0.04,0.06)
            Rectangle(pos=self.pos, size=self.size)
            # draw bars
            w = self.width / len(self.bands)
            for i, v in enumerate(self.bands):
                # color gradient
                Color(0.0 + 1.2*v, 0.5*(1-v), 0.6 + 0.4*(1-v), 1)
                h = v * self.height * 0.9
                Rectangle(pos=(self.x + i*w + w*0.08, self.y + self.height*0.05), size=(w*0.84, h))

class MainLayout(BoxLayout):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.orientation = 'vertical'
        self.visual = BarVisualizer(size_hint=(1,0.8))
        self.add_widget(self.visual)
        # sliders
        from kivy.uix.gridlayout import GridLayout
        grid = GridLayout(cols=5, size_hint=(1,0.2))
        self.sliders = []
        for i in range(10):
            s = Slider(min=-12, max=12, value=0)
            self.sliders.append(s)
            grid.add_widget(s)
        self.add_widget(grid)
        Clock.schedule_interval(self.update_visual, 1/30.)

    def update_visual(self, dt):
        # placeholder: random wobble for demo
        levels = np.clip(np.abs(np.sin(np.linspace(0,1,10) + time()*2.0)) * np.random.rand(10), 0, 1)
        # In real app, feed levels from audio analysis thread
        self.visual.set_levels(levels)

# Android audio thread using AudioRecord
class AndroidAudioThread(threading.Thread):
    def __init__(self, consumer_callback):
        super().__init__(daemon=True)
        self.consumer = consumer_callback
        self.running = True

    def run(self):
        if not ANDROID:
            return
        AudioRecord = autoclass('android.media.AudioRecord')
        AudioFormat = autoclass('android.media.AudioFormat')
        MediaRecorder = autoclass('android.media.MediaRecorder')
        min_buf = AudioRecord.getMinBufferSize(44100, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT)
        rec = AudioRecord(MediaRecorder.AudioSource.MIC, 44100, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, min_buf)
        rec.startRecording()
        import array
        buf = array.array('h', [0])*min_buf
        while self.running:
            n = rec.read(buf, 0, min_buf)
            if n>0:
                # convert to numpy and do FFT in Python
                data = np.frombuffer(buf, dtype=np.int16).astype(np.float32)/32768.0
                # compute bands (simple): send energy per band (this is simplified)
                spectrum = np.abs(np.fft.rfft(data * np.hanning(len(data))))
                freqs = np.fft.rfftfreq(len(data), 1/44100.)
                bands = []
                for f in [31,62,125,250,500,1000,2000,4000,8000,16000]:
                    idx = np.where((freqs >= f/np.sqrt(2)) & (freqs <= f*np.sqrt(2)))[0]
                    bands.append(np.mean(spectrum[idx]) if len(idx) else 0.0)
                bands = np.array(bands)
                bands = bands / (bands.max()+1e-12)
                self.consumer(bands)
        rec.stop(); rec.release()

# helper time
import time
def time():
    return time_module.time()

import time as time_module

class GlowEQApp(App):
    def build(self):
        self.main = MainLayout()
        # Kick off Android audio consumer if available
        if ANDROID:
            self.aud_thread = AndroidAudioThread(self.on_bands_from_audio)
            self.aud_thread.start()
        return self.main

    def on_bands_from_audio(self, bands):
        # called by audio thread
        def set_levels(dt=None):
            self.main.visual.set_levels(bands)
        Clock.schedule_once(set_levels, 0)

if __name__ == '__main__':
    GlowEQApp().run()
```

### buildozer.spec (minimal) â€” save as `buildozer.spec`
```
[app]
title = GlowEQ
package.name = gloweq
package.domain = org.example
source.dir = .
source.include_exts = py,png,jpg,kv,txt
version = 0.1
requirements = python3,kivy,pyjnius,numpy
# if you need other libraries add them here
android.permissions = RECORD_AUDIO

# build settings (use docker or Linux host) omitted details here â€” see Buildozer docs

[buildozer]
log_level = 2
```

### How to build (brief)
1. On Linux/WSL/Ubuntu install dependencies for Buildozer (see https://buildozer.readthedocs.io). Best on a clean Ubuntu VM.
2. `pip install buildozer` and `sudo apt-get install -y ...` as docs say.
3. Place `main.py`, `buildozer.spec` in project folder.
4. Run `buildozer android debug deploy run` to build & install on a connected Android device.

If you prefer CI: use GitHub Actions with a Ubuntu runner and the official buildozer/docker image to produce the APK.

---

## Summary & next steps I can do right now
- I updated the desktop code above â€” copy `python_equalizer_visual.py` and run it to get the upgraded visuals.
- For Android I provided a **Kivy template** + `buildozer.spec` and step-by-step notes. I cannot compile the APK here, but the template includes `pyjnius` usage for AudioRecord and is ready to build on your machine or CI.

Which do you want next? I can:
- further tune the colors/decay/glow parameters in the desktop code and produce alternative themes, OR
- expand the Android template with a complete JNI-backed high-performance audio pipeline and a ready-to-use `buildozer.spec` for a specific Android API level (I'll produce the full spec and instructions).
